# RL Poker：基于 GPU 向量化与对手池的四人 Big Two 强化学习系统技术报告

## 摘要

本文对 RL Poker 项目进行系统化分析与技术总结，聚焦于四人 Big Two 变体扑克的强化学习建模、GPU 向量化环境设计、固定动作空间与掩码机制、PPO 训练流程、PSRO-lite 对手池采样策略，以及行为信念与记忆模块的工程实现。项目通过 CPU 规则引擎保证规则正确性，使用 GPU 批量环境与预枚举动作空间提升训练吞吐，并在训练阶段引入对手池动态采样与形势信念估计，形成“高性能模拟 + 多智能体对抗 + 结构化先验”的训练框架。本文给出模块化架构梳理、关键算法流程、状态/动作编码细节、评估指标与可复现实验路径，并讨论当前方法的局限与改进方向，为后续研究或复现实验提供技术依据与工程参照。

## 关键词

强化学习；多智能体；扑克；Big Two；GPU 向量化；PPO；对手池；信念建模

## 1 引言

多智能体扑克类任务具有不完全信息、组合动作空间大、长时序博弈等特征。经典的基于规则或枚举的对弈系统难以在大规模对局与策略迭代中保持训练效率。RL Poker 项目提出一套工程化解决方案：以 CPU 规则引擎保证合法性与可解释性，以 GPU 向量化环境和固定动作空间提升吞吐，以 PPO 为基础策略优化器，并结合对手池（PSRO-lite）与行为信念特征应对自博弈坍塌与策略弱点暴露问题 [1][3][4][5]。本文对其设计与实现进行深入分析，并整理为技术报告。

## 2 游戏规则与任务定义

RL Poker 实现了四人 Big Two 变体，核心规则如下 [1][2]：

- 4 人对局，每人 13 张牌（标准 52 张）
- 持有红桃 3 的玩家先手，首出必须包含红桃 3
- 牌型包括单张、对子、顺子（5+ 连牌，仅 3-K）、连对（3+ 连对，仅 3-K）、三带二、四带三
- 尾牌豁免规则：当玩家剩余牌数满足尾牌条件时，三带二可用 3+0/3+1，四带三可用 4+0/4+1/4+2，且必须一次出完手牌
- 若上一手使用豁免，下一手只能用标准牌型（3+2 为 5 张，4+3 为 7 张）或不出
- 3 名玩家出完牌后结束，计分为 +2、+1、-1、-2（名次 1 到 4）

该任务可建模为四人轮流行动的部分可观测马尔可夫博弈（POMG），并以终局得分为主要回报信号。

## 3 系统架构

项目采用“规则引擎 + 环境封装 + 训练/评估 + 交互界面”的分层结构：

- 规则层：CPU 规则引擎与合法出牌枚举，保证规则一致性与可测试性 [1][2]
- 环境层：PettingZoo AEC 环境用于多智能体接口与基线评估 [7]
- GPU 训练层：GPUPokerEnv 实现批量并行对局、动作掩码与回合推进 [4]
- 学习层：PPO 训练脚本与评估脚本，集成对手池与信念特征 [5][6]
- 交互层：TUI 与 Web UI 用于人机交互或演示

此结构兼顾正确性（CPU）与吞吐（GPU），并以模块边界支撑扩展与实验复用。

## 4 方法

### 4.1 状态表示

GPU 环境的基础观测由以下部分拼接而成 [4]：

- 手牌 52 维 one-hot
- 牌阶计数 13 维（每个牌阶 0-4）
- 上下文 5 维（上一手动作、上一手主牌阶、连续不出次数、是否领出、桌面剩余牌归一化）

训练时进一步拼接结构化信念特征 [5]：

- 对手可能手牌分布的“信念向量”（3×13）
- 对手剩余牌数（3）
- 公共已出牌阶计数（13）

该设计以公共可观测动作与出牌统计为先验，缓解不完全信息带来的观测缺口。

### 4.2 动作空间与掩码

GPU 训练采用固定动作空间，预先枚举所有可能动作并在运行时生成合法掩码 [3]：

- PASS
- 单张、对子
- 顺子（5+ 连牌）
- 连对（3+ 连对）
- 三带二、四带三
- 尾牌豁免动作

合法性判断通过“按牌阶计数是否满足需求”实现，并叠加“首出必须包含红桃 3”和“跟牌必须大于上一手”的约束 [3][4]。该机制避免了每步 CPU 组合枚举，显著提高吞吐。

### 4.3 PPO 训练

策略优化采用 PPO，配合 GAE 计算优势 [5]：

$$
\hat{A}_t = \sum_{l=0}^{T-t-1}(\gamma\lambda)^l \delta_{t+l},\quad \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

更新目标为：

$$
L^{\text{PPO}} = \mathbb{E}\left[\min\left(\rho_t\hat{A}_t, \text{clip}(\rho_t, 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]
$$

其中动作掩码用于屏蔽非法动作。训练收集“学习者行动步”，对手在学习者行动之间推进环境，避免无效的重复决策记录。

### 4.4 PSRO-lite 对手池

训练对手来自动态对手池，包含随机策略、启发式策略与历史快照 [5]。对手强度通过学习者 EV 的指数滑动均值（EMA）估计，采样权重按如下方式计算：

$$
\text{pressure}_i = \max(-\text{EV}_i, 0),\quad w_i \propto \exp(\beta\,\text{pressure}_i)
$$

同时引入最小采样概率，避免对手覆盖不足。该机制鼓励学习者持续对抗“更难打”的策略。

### 4.5 行为信念与记忆

项目使用行为信念更新对手牌阶分布 [5]：

- 当对手出牌时，依据动作需求对牌阶分布施加正向证据
- 当对手不出时，根据“可回应动作的牌阶权重”施加惩罚
- 通过牌阶相似度核（rank affinity）进行平滑

同时使用历史缓冲区与 GRU 处理动作序列，历史特征由“玩家 ID one-hot、动作类型 one-hot、主牌阶/长度归一化、是否豁免、是否 PASS”等组成 [5]。

### 4.6 奖励与 shaping

环境终局回报为名次得分（+2、+1、-1、-2）。训练阶段引入形势 shaping：

- 以对手池 EV 作为基线
- 仅在对局结束时，将 shaping 奖励回填到学习者的最后一步
- shaping 系数随更新次数线性退火

该设计可缓解稀疏回报下的优化困难，同时保留最终目标一致性 [5]。

## 5 实现细节与工程要点

- GPU 环境 `GPUPokerEnv` 完成发牌、回合推进、胜负判断与批量重置 [4]
- `GPUActionMaskComputer` 以预枚举动作表与“按牌阶需求计数”生成合法掩码 [3]
- CPU 规则引擎与 GPU 合法性通过单元测试对齐，确保一致性 [1][2]
- PettingZoo AEC 环境提供标准多智能体接口，支持外部评估或基线对照 [7]
- 评估脚本支持对单模型或一组 checkpoint 的评测，并输出 JSON 结果与 Elo [6]

## 6 评估流程与指标

评估主要通过 GPU 评估脚本完成 [6]：

- 平均得分、胜率、平均名次
- Elo 评级（四人多对多形式）
- 评估结果可写入 `eval_results.json` 并用于 checkpoint 筛选脚本

由于本报告基于代码级分析，不包含固定的实验数值。实际效果取决于训练时长、对手池配置、GRU/信念参数等超参组合。

## 7 讨论与局限

- 对手池基线以随机与启发式策略为主，缺少外部强基线的交叉验证
- 信念更新采用启发式证据与平滑核，尚未进行严格的概率建模或校准
- 固定动作空间在极端组合下可能存在冗余动作，仍需精细化剪枝与统计验证
- 当前 reward shaping 与 PSRO-lite 仍是工程启发式，缺少理论收敛保证
- 评价指标以胜率/平均名次为主，对策略多样性与稳定性刻画不足

## 8 结论

RL Poker 项目通过“CPU 规则正确性 + GPU 高吞吐模拟 + PPO 优化 + PSRO-lite 对手池 + 行为信念与记忆”构建了一套可扩展的多智能体扑克训练框架。其工程实现强调吞吐与规则一致性，在不完全信息环境下通过结构化先验与对手池对抗提升策略质量。未来可在更强基线、信念建模、策略多样性评估与理论分析方面扩展，以进一步验证与提升系统能力。

## 参考文献

[1] `rl_poker/engine/game_state.py`

[2] `rl_poker/moves/legal_moves.py`

[3] `rl_poker/moves/gpu_action_mask.py`

[4] `rl_poker/rl/gpu_env.py`

[5] `rl_poker/scripts/train.py`

[6] `rl_poker/scripts/eval_gpu.py`

[7] `rl_poker/envs/rl_poker_aec.py`
