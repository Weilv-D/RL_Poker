Metadata-Version: 2.4
Name: rl_poker
Version: 0.1.0
Summary: Reinforcement Learning for Poker
Author: RL Poker Team
License: MIT
Keywords: reinforcement-learning,poker,pettingzoo,gymnasium
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: pettingzoo>=1.24.0
Requires-Dist: gymnasium>=0.29.0
Requires-Dist: numpy>=1.24.0
Requires-Dist: tensorboard>=2.14.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"

# RL Poker

四人扑克规则的强化学习项目，包含完整规则引擎、PettingZoo AEC 环境，以及 GPU 向量化训练脚本。

## 特性

- **规则引擎**: 严格实现牌型、出牌、豁免与结算逻辑
- **AEC 环境**: PettingZoo AEC 轮流出牌 + 动作掩码
- **PPO 训练**: GPU 向量化训练脚本（完整规则）
- **TensorBoard**: 训练日志可视化

## 游戏规则（完整规则引擎与 AEC 环境）

- 4 人，52 张牌，每人 13 张；不含大小王
- 点数大小：2 > A > K > Q > J > 10 > 9 > 8 > 7 > 6 > 5 > 4 > 3
- 不比花色，只比点数
- 无炸弹：四张相同仅作为四带三使用

**合法牌型**
- 单张
- 对子
- 顺子：5 张及以上，仅 3-K（A/2 禁止）
- 连对：3 对及以上，仅 3-K（A/2 禁止）
- 三带二
- 四带三

**尾牌豁免（仅尾牌）**
- 三带二：剩 4 张可出 3+1；剩 3 张可出 3+0
- 四带三：剩 6 张可出 4+2；剩 5 张可出 4+1；剩 4 张可出 4+0
- 上家豁免后，下家必须补足标准张数才能压（否则必须 PASS）

**流程与结算**
- 红桃 3 持有者首出，且首手必须包含红桃 3
- 轮流出牌，不能压则 PASS
- 第三名出完牌即结束
- 积分：第 1 名 +2，第 2 名 +1，第 3 名 -1，第 4 名 -2

## 训练脚本规则说明（GPU 向量化训练）

`rl_poker/scripts/train.py` 使用 GPU 向量化环境以追求高吞吐，规则与完整引擎一致：

- **胜负**：第三名出完牌即结束
- **奖励**：第 1 名 +2，第 2 名 +1，第 3 名 -1，第 4 名 -2
- **尾牌豁免**：三带二与四带三的尾牌豁免规则已实现
- **顺子/连对长度**：顺子 5-11；连对 3-11 对（仅 3-K）
- **首手限制**：红桃 3 持有者首出，且首手必须包含红桃 3

## 安装

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .
pip install tensorboard  # 可选
```

## 训练

```bash
# 快速测试
python -m rl_poker.scripts.train --total-timesteps 100000

# 带 TensorBoard 的训练脚本
./scripts/train_gpu.sh --total-timesteps 1000000
```

### 监控训练

```bash
tensorboard --logdir runs
# http://localhost:6006
```

### 训练参数（train.py）

```
--num-envs N           并行环境数量 (默认: 256)
--total-timesteps N    总训练步数 (默认: 1000000)
--rollout-steps N      每次 rollout 的步数 (默认: 128)
--hidden-size N        网络隐藏层大小 (默认: 256)
--learning-rate LR     学习率 (默认: 3e-4)
--gamma GAMMA          折扣因子 (默认: 0.99)
--gae-lambda LAM       GAE lambda (默认: 0.95)
--ppo-epochs N         PPO 更新轮数 (默认: 4)
--num-minibatches N    小批次数量 (默认: 4)
--clip-coef C          PPO clip 系数 (默认: 0.2)
--ent-coef C           熵奖励系数 (默认: 0.01)
--vf-coef C            价值损失系数 (默认: 0.5)
--max-grad-norm N      梯度裁剪 (默认: 0.5)
--seed N               随机种子 (默认: 42)
--checkpoint-dir DIR   检查点目录 (默认: checkpoints)
--log-dir DIR          日志目录 (默认: runs)
--no-cuda              禁用 CUDA
```

## 评估（基于 AEC 完整规则）

```bash
python -m rl_poker.scripts.evaluate --episodes 50 --opponents random,heuristic
```

## 项目结构

```
rl_poker/
├── rules/              # 游戏规则定义
│   ├── ranks.py        # 牌点和花色定义
│   └── hands.py        # 牌型判断和比较
├── moves/              # 动作空间与掩码
│   ├── legal_moves.py  # 合法动作枚举（完整规则）
│   ├── action_encoding.py
│   └── gpu_action_mask.py  # GPU 向量化训练动作掩码
├── engine/             # 游戏引擎（完整规则）
│   └── game_state.py
├── envs/               # 环境实现
│   └── rl_poker_aec.py
├── scripts/            # 训练和评估脚本
│   ├── train.py
│   ├── evaluate.py
│   └── smoke_env.py
├── agents/
│   ├── random_agent.py
│   └── heuristic_agent.py
└── utils/
    └── seeding.py

scripts/
└── train_gpu.sh

tests/
```

## 开发

```bash
python -m pytest
```

## License

MIT License
